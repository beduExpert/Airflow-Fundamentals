# Sesi√≥n 2: Conceptos B√°sicos ü§ñ

## 1. Objetivos

- Crear el primer archivo DAG en python y ejecutarlo
- Usar el operador BashOperator para ejecutar comandos BASH
- Usar el operador PythonOperator para llamar funciones Python
- Definir las dependencias entre tareas
- Monitorear la ejecuci√≥n del DAG y revisar los resultados de las tareas

## 2. Contenido

Vamos a conectar cada uno de las temas de esta sesi√≥n en un s√≥lo ejercicio a trav√©s de un caso de uso

![apod](Ejemplo-01/assets/img/mosaico.png)

### Caso de Uso: Salvapantallas con fotos de la NASA

Digamos que nos gustar√≠a recopilar esas impresionantes fotograf√≠as del espacio que obtiene la NASA. Adem√°s de las fotograf√≠as tambi√©n estamos interesados en obtener los metadatos de la misma: el t√≠tulo, la descripci√≥n, etc.

Utilizaremos una de las APIs abiertas de la NASA: [APOD](https://api.nasa.gov/).

Uno de los sitios web m√°s populares de la NASA es la [imagen astron√≥mica del d√≠a](http://apod.nasa.gov/apod/astropix.html).

La documentaci√≥n completa de esta API se puede encontrar en el repositorio [Github de la API de APOD](https://github.com/nasa/apod-api).

- No necesita autenticarse para explorar los datos de la NASA.
- En los ejemplos utilizaremos la clave de API especial `DEMO_KEY`. Esta clave de API se puede usar para explorar inicialmente las API antes de registrarse, pero tiene l√≠mites de frecuencia mucho m√°s bajos,
  - L√≠mite por hora: 30 solicitudes por direcci√≥n IP por hora
  - L√≠mite diario: 50 solicitudes por direcci√≥n IP por d√≠a


![apod](Ejemplo-01/assets/img/flujo_apod2.png)

Lo primero que tenemos que hacer es desglosar el proceso en tareas, sabemos que debemos descargar la metadata, la fotograf√≠a y tambi√©n vamos a agregar una √∫ltima tarea para contabilizar el n√∫mero de fotograf√≠as que hemos descargado hasta el momento.

Un posible flujo de tareas ser√≠a el siguiente:

![graph view](Ejemplo-01/assets/img/zoom_apod_graph_view.png)
> Airflow utiliza un c√≥digo de colores para representar cada tipo de operador.

1. Descargar los metadatos a trav√©s de una API p√∫blica
2. Descargar la fotograf√≠a del d√≠a
3. Contabilizar el total de fotograf√≠as descargadas hasta el momento

Para definir este pipeline usaremos dos de los operadores que son parte del n√∫cleo de Airflow: *BashOperator* y *PythonOperator*.

- [BashOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html) se utiliza para ejecutar comandos de linux utilizando el int√©rprete BASH.
- [PythonOpeator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html). se usa para ejecutar llamadas de funci√≥n de Python.

 Iremos implementando la soluci√≥n de manera gradual, poniendo en pr√°ctica cada uno de los temas estudiados previamente. Nos apoyaremos de algunos retos a lo largo de la implemntaci√≥n para reforzar el conocimiento adquirido.

### Preparaci√≥n

1. Abrir una terminal
2. Cambiarse al directorio de trabajo de airflow, por ejemplo:

  ```bash
  cd ~/repo/airflow
  ```

3. Este directorio debe contener el archivo `docker-compose.yaml` y las tres carpetas que creamos en la sesi√≥n anterior: `dags`, `plugins` y `logs`


4. Dentro de VS Code, activamos la terminal **View > Terminal**
5. Activamos nuestro ambiente virtual con Python3.7 o Python3.10
6. Instalamos el m√≥dulo de apache airflow si no lo hemos instalado antes

  ```bash
  pip install "apache-airflow[celery]==2.5.1" \
          --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.1/constraints-3.7.txt"
  ```

7. Abrimos la paleta de comandos **View > Command Palette...**, empezamos a escribir `Python: Select Interpreter` y seleccionamos el nombre de nuestro ambiente virtual de Python.

 > Esta paso garantiza que VS Code reconozca los m√≥dulos de airflow, muestre la ayuda y el autocompletado asumiendo que el [paquete de extension para Python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) este instalado.

6. Levantamos los servicios de airflow
  
  ```bash
  docker-compose up
  ```

7. Verificamos que todos los servicios aparezcan con el estado `healthy`

```bash
docker ps
```

8. Abrimos un navegador con la direcci√≥n [http://localhost:8080](http://localhost:8080) y comprobamos que el servidor web funciona correctamente.

 ### Paso 1. Creaci√≥n de un archivo DAG

1. Abrimos Visual Studio Code (VS Code)
2. Seleccionamos la opci√≥n del men√∫ **File > Open Folder...**
3. Ubicamos nuestra carpeta de trabajo y seleccionamos. Se trata de la misma carpeta que contiene el archivo `docker-compose.yaml`
4. Abrimos el explorador de archivos y creamos un archivo dentro de la carpeta `dags` con el nombre `apod.py`

### Paso 2. Declaraci√≥n del DAG

Sabemos que existen tres formas diferentes de declarar un dag, en esta ocasi√≥n usaremos el constructor

1. Lo primero que tenemos que hacer es importar el m√≥dulo de Airflow que contiene la clase DAG.

  ```python
  from airflow import DAG
  import pendulum
  ```

2. Despu√©s creamos un objeto DAG usando el constructor

  ```python
  dag = DAG( ‚ë†
    dag_id="ejemplo1_apod", ‚ë° 
    description="Ejemplo 1. NASA's Astronomy Picture of the Day",                    
    start_date=pendulum.datetime(2023, 1, 15, tz="UTC"),‚ë¢   
    schedule=None ‚ë£        
  )
  ```

- ‚ë† Instancia del objeto DAG
- ‚ë° Nombre del DAG
- ‚ë¢ Marca el inicio del primer intervalo del DAG
- ‚ë£ El intervalo de ejecui√≥n que usar√° el Scheduler para planear las ejecuciones autom√°ticas

3. Guardamos nuestro archivo DAG
4. Abrimos Airflow en el navegador y buscamos el dag con el nombre `ejemplo1_apod`

  > PROBLEMA: Aunque nuestro DAG no produce ning√∫n error de sintaxis, no se mostrar√° en la interfaz hasta que tenga asociado al menos una tarea.

  > SOLUCI√≥N: Utilizaremos el operador EmptyOperator, que literalmente no realiza ninguna acci√≥n, comunmente se usa para agrupar otras tareas para resolver este inconveniente.

5. Importamos el operador `EmptyOperator`

```python
from airflow.operators.empty import EmptyOperator
```

6. Instanciamos el operador y lo asociamos a nuestro dag

```python
EmptyOperator(task_id='empty', dag=dag)
```
7. Si todo va bien, esta vez el DAG `ejemplo1_apod` aparecer√° en la p√°gina principal de Airflow

8. Para verificar que tu DAG no contiene errores de sintaxis ni de dependencias, puedes ejecutar en la terminal:

  ```bash
  python tu-archivo-dag.py
  ```

[parte1_apod.py](Ejemplo-01/assets/dags/parte1_apod.py)

---
### [Reto 1. Declaraci√≥n alternativa de DAGs](Reto-01/README.md)

---

### Paso 3. Uso del operador `BashOperator`

Utilizaremos el operador `BashOperator` para hacer una petici√≥n a la API p√∫blica de la NASA. Como sabemos este operador nos permite ejecutar los comandos que normalmente ejecutamos en nuestra terminal.

Para interactuar con la API, utilizaremos el comando `curl` para hacer la solicitud GET y guardar la respuesta `JSON` en un archivo.

```bash
curl -o apod.json -L 'https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY
```

> TIP: Recuerda que si necesitas consultar la documentaci√≥n de cualquier comando puedes agregar el par√°metro `--help`.

```bash
curl --help
Usage: curl [options...] <url>
 -d, --data <data>          HTTP POST data
 -f, --fail                 Fail silently (no output at all) on HTTP errors
 -h, --help <category>      Get help for commands
 -i, --include              Include protocol response headers in the output
 -o, --output <file>        Write to file instead of stdout
 -O, --remote-name          Write output to a file named as the remote file
 -s, --silent               Silent mode
 -T, --upload-file <file>   Transfer local FILE to destination
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>    Send User-Agent <name> to server
 -v, --verbose              Make the operation more talkative
 -V, --version              Show version number and quit
```

Ejemplo del contenido del archivo `apod.json`

```json
{"copyright": "Petr HoralekInstitute of Physics in Opava",
 "date": "2023-02-07",
 "explanation":"Can you still see the comet? Yes.Even as C/2022 E3 (ZTF)",
 "hdurl": "https://apod.nasa.gov/apod/image/2302/ZtfDippersB_Horalek_960_annotated.jpg",
 "media_type": "image",
 "service_version": "v1",
 "title": "A Comet and Two Dippers",
 "url":"https://apod.nasa.gov/apod/image/2302/ZtfDippersB_Horalek_960_annotated.jpg"}
```

La respuesta incluye el t√≠tulo, la fecha y una breve explicaci√≥n de la fotograf√≠a. Tambi√©n podemos encontrar dos ligas URL que hacen referencia la imagen en dos diferentes resoluciones.

Ahora regresemos a nuestro archivo DAG, antes de usar cualquier operador debemos importarlo.

1. Agregamos el m√≥dulos de Airflow que contienen la clase `BashOperator`.

  ```python
  from airflow.operators.bash import BashOperator
  ```

2. Para crear nuestra primera tarea, instanciamos de la clase `BashOperator`

  > En Airflow, la instancia de una clase operador se conoce como tarea.

```python
descarga_info = BashOperator(
  task_id="descarga_info", ‚ë†
  bash_command="curl -o galeria/metadata/apod.json -L 'https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY'" ‚ë°  
)
```

1. El nombre de la tarea
2. El comando BASH

[parte2_apod.py](Ejemplo-01/assets/dags/parte2_apod.py)

---
### [Reto 2. Foto de cumplea√±os](Reto-02/README.md)
---

### Paso 3. Uso del operador `PythonOperator`

En este paso crearemos una funci√≥n en Python para leer el archivo `JSON` con los metadatos, despu√©s extraemos la url de la imagen, y usamos `requests` para descargar la imagen y guardarla en un archivo local.

1. Importamos las dependencias usadas por nuestra funci√≥n

```python
import logging
import json
import pathlib
import requests
import requests.exceptions as requests_exceptions
```

2. Agregamos la definici√≥n de la funci√≥n `_descarga_foto`

  ```python
  def _descarga_foto():
      # creamos la carpeta destino si no existe
      OUTPUT_DIR = '/opt/airflow/dags/sesion02/pictures'
      INPUT_DIR = '/opt/airflow/dags/sesion02/metadata'
      pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
      # recuperamos los metadatos descargados en el paso anterior
      with open(f'{INPUT_DIR}/apod.json', 'r') as f:
          info = json.load(f)
          image_url = info['url']        
          try:
              image_filename = image_url.split("/")[-1]
              target_file = f'{OUTPUT_DIR}/{image_filename}'
              logging.info(f"Descargando la foto del d√≠a {image_url} al archivo local: {target_file}")

              response = requests.get(image_url, timeout=100)
              with open(target_file, "wb") as f:
                  f.write(response.content)            
          except requests_exceptions.ConnectionError:
              logging.error(f"Error de conexi√≥n {image_url}.")
      return info['title']
  ```

3. Importamos la clase `PythonOperator`

  ```python
  from airflow.operators.python import PythonOperator
  ```

4. Ahora creamos una instancia del operador `PythonOperator` y usamos la propiedad `python_callable` para hacer referencia al nombre de la funci√≥n que queremos ejecutar, en este caso nuestra funci√≥n `_descarga_foto`

  ```python
  descarga_foto = PythonOperator(
    task_id="descarga_foto",
    python_callable=_descarga_foto
  )
  ```

[parte3_apod.py](Ejemplo-01/assets/dags/parte3_apod.py)

### Paso 4. Usamos `BashOperator` para saber cu√°ntas fotos tenemos en nuestra galer√≠a

Esta vez usaremos dos comandos encadenados, `ls` para listar el contenido del directorio y `wc -l` para contar el n√∫mero de lineas de la salida de `ls`.


```python
OUTPUT_DIR = '/opt/airflow/dags/sesion02/pictures'
notifica = BashOperator(
   task_id="notifica",
 bash_command=f'echo "Ahora son $(ls {OUTPUT_DIR} | wc -l) fotograf√≠as."'
)
```

[parte4_apod.py](Ejemplo-01/assets/dags/parte4_apod.py)

### Paso 5. Definici√≥n de dependencias

Por √∫ltimo definimos las dependencia entre tareas utilizando el operador de desplazamiento de bit

```python
descarga_info >> descarga_foto >> notifica
```

---
### [Reto 3. Definici√≥n de dependencias y ejecuci√≥n programada](Reto-03/README.md)
---


### 3. Postwork

- [**`POSTWORK SESI√ìN 2`**](Postwork/README.md)
